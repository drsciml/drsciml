<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta property="og:image" content="https://drsciml.github.io/drsciml/assets/img/Operators.png" />

  <title>DRSciML</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Lato:400,300,700,900"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Amoeba - v2.0.0
  * Template URL: https://bootstrapmade.com/free-one-page-bootstrap-template-amoeba/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
          onload="renderMathInElement(document.body, {
              delimiters: [
                  {left:'$', right:'$', display:false},
                  {left:'$$', right:'$$', display:true}
              ]
          });"></script>
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container">

      <div class="logo float-left">
        <h1 class="text-light"><a href="index.html"><span>DRSciML 2025</span></a></h1>
        <!-- Uncomment below if you prefer to use an image logo -->
        <!--a href="index.html"><img src="assets/img/chatbot.jpg" alt="" class="img-fluid"></a-->
      </div>

      <nav class="nav-menu float-right d-none d-lg-block">
        <ul>
          <li class="active"><a href="#header">Home</a></li>
          <li><a href="#about">Description</a></li>
          <!-- <li><a href="#Papers">Call for Papers</a></li>
          <li><a href="#Submission">Submission</a></li>
          <li><a href="#dates">Dates</a></li> -->
          <li><a href="#schedule">Programme</a></li>
          <li><a href="#speakers">Speakers</a></li>
          <li><a href="#talks">Talks</a></li>
          <li><a href="#team">Organizers</a></li>
          <!-- <li><a href="#Registration">Registration</a></li> -->
          <li><a href="#location">Location</a></li>
          <!-- <li><a href="#previous-events">Previous Events</a></li> -->
          <li><a href="#contact">Contact Us</a></li>
        </ul>
      </nav><!-- .nav-menu -->

    </div>
  </header><!-- End #header -->

  <style>
    /* Ensure that the header is fixed and covers the width of the page */
    #header {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      z-index: 1030;
      /* Adjust z-index if necessary to ensure the header is above other content */
    }

    /* Style the container within the header to ensure it spans the full width */
    #header .container {
      width: 100%;
      max-width: 100%;
      /* You might need to adjust this depending on your layout */
      padding-right: 15px;
      padding-left: 15px;
      margin-right: auto;
      margin-left: auto;
    }

    /* Adjust the nav-menu to ensure all items fit */
    .nav-menu ul {
      display: flex;
      justify-content: space-around;
      /* This will space your menu items evenly */
      list-style: none;
      padding-left: 0;
    }
  </style>

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container">
      <h1><b>Surrogates and Dimension Reduction in Scientific Machine Learning</b></h1>
      <h2><b>9th-10th September, 2025 </b></h2>
      <h2><b>Hybrid Event in the University of Manchester, United Kingdom</b></h2>

      <a href="https://www.eventbrite.com/e/surrogates-and-dimension-reduction-in-scientific-machine-learning-tickets-1609553230039?aff=oddtdtcreator" target="_blank" class="btn-get-started scrollto">Register</a>
      </div-->
  </section><!-- #hero -->

  <main id="main">

    <!-- ======= About Us Section ======= -->
    <section id="about" class="about">
      <div class="container">
        <div class="section-title">
          <h2>About DRSciML-2025</h2>
        </div>
        <div class="row">
          <p class="description">
            Scientific machine learning has increasingly focused on surrogates for solving partial differential
            equations, modeling dynamical systems, and learning physical phenomena from data. These surrogates offer
            efficient and scalable alternatives to traditional methods, with significant potential across diverse
            applications. However, challenges persist in ensuring stability and providing robust theoretical guarantees.
            This workshop will explore recent advances in surrogate modeling, emphasizing dimension reduction, effective
            training strategies, and methods that enhance accuracy and theoretical rigor.
          </p>
        </div>
      </div>
    </section>
    <!-- End About Us Section -->

    <!-- ======= CFP Section ======= -->
    <!-- <section id="Papers" class="papers">
      <div class="container">
        <div class="section-title">
          <h2>Call for Papers</h2>
        </div>
        <div class="row">
          <p class="description">
            NSG 2024 invites authors to submit papers on research that exploits different NLP techniques to address
            different social issues (e.g. education, law, healthcare, climate change or any priorities in SDG).
          </p>
          <p class="description">
            Topics of interest include but are not limited to:
          </p>
          <ul>
            <li>Applications and solutions on different topics corresponding to NLP for Social Good (NSG) theme.</li>
            <li>Natural Language Interfaces and Interaction: design and implementation of Natural Language Interfaces,
              user studies with human participants on issues related to NSG topics.</li>
            <li>Large Language Models & Vision Language Models: Opportunities and Risks of using LLMs and VLMs in NSG.
            </li>
            <li>eXplainable Artificial Intelligence (XAI): Scope of Interpretability in NSG topics.</li>
            <li>Corpus/Dataset Analysis: Corpus or dataset collection or analysis for NSG related topics</li>
            <li>Multi-modal solutions to the theme of NSG.</li>
            <li>Unique proposition and contribution in any 17 goals of SDG using NLP.</li>
          </ul>
        </div>
      </div>
    </section> -->
    <!-- End CFP Section -->

    <!-- ======= Submission Section ======= -->
    <!-- <section id="Submission" class="submission">
      <div class="container">
        <div class="section-title">
          <h2>Submission Guidelines</h2>
        </div>
        <p class="description">
          Authors should follow the <a
            href='https://www.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw'
            target="_blank">CEUR single-column
            conference format</a> and submit their manuscripts in pdf via <a
            href='https://easychair.org/my/conference?conf=nsg2024' target="_blank">Easychair conference page</a>
          (submissions are NOW OPEN).
          Submissions must be 2 to 8 pages of content (plus any number of additional pages for reference).The review
          process
          is <strong style="color: #70b9b0;">double-blind</strong>. All questions about submissions should be emailed
          to <a href="mailto:nlp4socialgood@gmail.com">nlp4socialgood@gmail.com</a>
        </p>
      </div>
    </section> -->
    <!-- End Submission Section -->

    <!-- <section id="dates" class="portfolio section-bg">
      <div class="container">
        <div class="section-title">
          <h2>Important Dates</h2>
        </div>
        <div class="row">
          <div class="col-lg-12">
            <ul>
              <li>Paper submission deadline : <s>?</s></li>
              <li>Paper notification: <s>?</s></li>
              <li>Camera-ready deadline: <s>?</s></li>
              <li>SDRSciML-2025: 9th-10th September, 2025</li>
            </ul>
          </div>
        </div>
      </div>
    </section> -->

    <!-- <section id="schedule" class="portfolio section-bg">
      <div class="container">
        <div class="section-title">
          <h2>DRSciML-2025 Programme</h2>
        </div>

        <h4 style="color:#70b9b0;">9th September 2025 - Tuesday</h4>
        <table class="table table-striped">
          <tbody>
            <tr>
              <td>08:45 - 11:00 BST</td>
              <td>Registration</td>
            </tr>
            <tr>
              <td>09:00 - 10:00 BST</td>
              <td>Plenary Talk - <a href="https://georgestepaniants.com/">George Stepaniants</a></td>
            </tr>
            <tr>
              <td>11:15 - 11:35 BST</td>
              <td>Talk 1 - <b>Inclusive by Design...</b></td>
            </tr>

          </tbody>
        </table>

        <h4 style="color:#70b9b0;">10th September 2025 - Wednesday</h4>
        <table class="table table-striped">
          <tbody>
            <tr>
              <td>15:00 - 16:00 BST</td>
              <td>Keynote - <a href="https://www.lrdc.pitt.edu/people/researcher-detail.cshtml?id=32">Prof. Kevin D. Ashley</a></td>
            </tr>

          </tbody>
        </table>
      </div>
    </section> -->

    <section id="schedule" class="portfolio section-bg">
      <div class="container">
        <div class="section-title">
          <h2>DRSciML-2025 Programme</h2>
        </div>

        <!-- Day 1 -->
        <div class="timeline-day">
          <h4 class="timeline-date">9th September 2025 - Tuesday</h4>

          <div class="timeline-event break">
            <span class="time">08:45 - 09:00 BST</span>
            <span class="desc">Registration</span>
          </div>

          <div class="timeline-event plenary">
            <span class="time">09:10 - 10:00 BST</span>
            <span class="desc">
              <div class="plenary-speaker">
                Plenary Talk - <a href="https://georgestepaniants.com/" target="_blank">George Stepaniants</a>
              </div>
              <div class="plenary-title">
                <span class="title-label">Title:</span> Learning Memory and Material Dependent Constitutive Laws
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  The simulation of multiscale viscoelastic materials poses a significant challenge in computational materials science, requiring expensive numerical solvers that can resolve dynamics of material deformations at the microscopic scale. 
                  The theory of homogenization offers an alternative approach to modeling, by locally averaging the strains and stresses of multiscale materials. This procedure eliminates the smaller scale dynamics but introduces a history dependence between strain and stress that proves very challenging to characterize analytically. 
                  In the one-dimensional setting, we give the first full characterization of the memory-dependent constitutive laws that arise in multiscale viscoelastic materials. Using this theory, we develop a neural operator architecture, that simultaneously across a wide range of material microstructures, accurately predicts their homogenized constitutive laws, thus enabling us to simulate their deformations under forcing. 
                  We use the approximation theory of neural operators to provide guarantees on the generalization of our approach to unseen material samples.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">10:00 - 10:30 BST</span>
            <span class="desc">Coffee Break</span>
          </div>

          <div class="timeline-event talk">
            <span class="time">10:30 - 11:00 BST</span>
            <span class="desc">Talk - <a href="https://scholar.google.com/citations?user=jcabNR0AAAAJ&hl=en" target="_blank">Fernando Henríquez</a></span>
          </div>

          <div class="timeline-event talk">
            <span class="time">11:00 - 11:30 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://scholar.google.com/citations?user=DN9CCpkAAAAJ&hl=sr" target="_blank">Bogdan Raonić (online)</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Operator Learning for PDEs
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  In this talk, I will present a series of recent works that explore the intersection of deep learning and scientific computing, with a particular focus on operator learning and generative modeling for PDEs. 
                  I will begin with our work on Convolutional Neural Operators (CNOs), which reinterprets operator learning through the lens of classical CNNs. I will then discuss Representation Equivalent Neural Operators (ReNO), where we address representational ambiguities and aliasing in operator learning. 
                  Building on these foundations, I will introduce Poseidon, a foundation model for time-dependent PDEs that leverages multiscale attention and temporal conditioning. 
                  Finally, I will present GenCFD, a diffusion-based generative model for statistical CFD, which captures uncertainty and high-order statistics in turbulent flows. Each of these works contributes a step toward building accurate, robust, and generalizable models for physical systems.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event talk">
            <span class="time">11:30 - 12:00 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="javascript:void(0)">Niklas Reinhardt</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Bayesian Operator Learning
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  We discuss a Bayesian framework for learning non-linear mappings in infinite-dimensional spaces. Given
                  a map $g_0:\mathcal{X}\to\mathcal{Y}$ between two separable Hilbert spaces, we study recovery of $g_0$ from $n\in\mathbb{N}$ noisy
                  input-output pairs $(\pmb{x},\pmb{y})=(X_i,Y_i)_{i=1}^n$ with $Y_i = g_0 (X_i ) + E_i$ ; here the $X_i\in\mathcal{X}$ represent randomly drawn
                  'design' points, and the $E_i$ are assumed to be i.i.d. draws from a Gaussian white noise process indexed by $\mathcal{Y}$. Choosing a suitable
                  'operator-valued' prior $\Pi_0$, we show well-posedness of the posterior $G|(\pmb{X},\pmb{Y})$ and establish convergence
                  rates for the posterior mean towards the ground truth in terms of the data size $n$.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">12:00 - 14:00 BST</span>
            <span class="desc">Lunch</span>
          </div>

          <div class="timeline-event talk">
            <span class="time">14:00 - 14:30 BST</span>
            <span class="desc">Talk - <a href="https://dibyakanti.github.io/" target="_blank">Dibyakanti Kumar</a></span>
          </div>

          <div class="timeline-event talk">
            <span class="time">14:30 - 15:00 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://tomoleary.github.io" target="_blank">Thomas O'Leary-Roseberry</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Optimization-Ready Surrogates with Derivative-Informed Training
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Modern decision-making for complex physical and engineered systems increasingly requires the ability to quantify high-dimensional uncertainties and make decisions by solving risk-averse optimization problems under uncertainty all in near real-time. 
                  Operator learning has emerged as a promising framework for enabling scalable surrogate modeling in this context. However, such approaches inevitably introduce approximation errors that can degrade the quality of downstream decisions.

                  In this talk, we explore how formulating operator learning tasks through the lens of decision-making objectives such as inverse problems and optimization under uncertainty can help mitigate these limitations. 
                  We derive a priori error bounds for these optimization tasks, which motivate operator learning formulations that explicitly penalize errors in the derivatives of input-output maps. 
                  We demonstrate the effectiveness of these methods on problems in structural health monitoring, shape optimization, and fluid flow control. Our approaches lead to (i) improved accuracy in surrogate-based optimization and (ii) empirically enhanced performance in statistical learning tasks.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event talk">
            <span class="time">15:00 - 15:30 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://romit-maulik.github.io/index.html" target="_blank">Romit Maulik (online)</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Scalable, adaptive, and explainable geometric deep learning with applications to computational physics
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  This work introduces an interpretable graph neural network (GNN) autoencoding framework for surrogate modeling of unstructured mesh fluid dynamics. The approach addresses two key challenges: latent space interpretability and compatibility with unstructured meshes. Interpretability is achieved through an adaptive graph reduction procedure that performs flowfield-conditioned node sampling, yielding masked fields that (i) localize latent activity in physical space, (ii) track the evolution of unsteady flow features such as recirculation zones and shear layers, and (iii) identify sub-graphs most relevant to forecasting. Compatibility with unstructured meshes is ensured through multi-scale message passing (MMP) layers, which extend standard message passing with learnable coarsening operations to efficiently reconstruct flowfields across multiple lengthscales. 
                  A regularization procedure further enhances interpretability by enabling error-tagging, i.e., the identification of nodes most associated with forecasting uncertainty. Demonstrations are conducted on large-eddy simulation data of backward-facing step flows at high Reynolds numbers, with extrapolations to ramp and wall-mounted cube configurations. 
                  The result is a new class of GNN autoencoders that combine predictive accuracy with physics-informed insight into both flow structures and model reliability.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">15:30 - 16:00 BST</span>
            <span class="desc">Coffee Break</span>
          </div>

          <div class="timeline-event plenary">
            <span class="time">16:00 - 16:50 BST</span>
            <span class="desc">
              <div class="plenary-speaker">
                Plenary Talk - <a href="https://people.epfl.ch/olga.fink?lang=en" target="_blank">Olga Fink</a>
              </div>
              <div class="plenary-title">
                <span class="title-label">Title:</span> From Physics to Machine Learning and Back: Applications in Dynamical Systems
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Deep learning has become an essential tool in many engineering applications. However, its effectiveness is often limited by its reliance on large, representative, and well-labeled datasets. 
                  In contrast, condition monitoring data from complex systems is typically sparse, unlabeled, and unrepresentative, making it difficult to apply purely data-driven methods effectively. 
                  Moreover, deep learning models often perform poorly in extrapolation scenarios—common in engineering systems with long service lives and evolving operational regimes.
                  To address these limitations, integrating physical laws and domain knowledge into deep learning frameworks has shown significant potential. This presentation will explore a range of approaches that integrate physics-based principles into machine learning models. 
                  Particular attention will be given to the use of structural inductive biases—such as those introduced by physics-informed graph neural networks—to improve model robustness, generalization and extrapolation.
                  Finally, the talk will examine emerging methods in symbolic regression that aim to close the loop between data-driven learning and physical understanding, enabling the discovery of interpretable, physics-consistent models from data.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">18:00 - 20:00 BST</span>
            <span class="desc">Conference Dinner</span>
          </div>
        </div>

        <!-- Day 2 -->
        <div class="timeline-day">
          <h4 class="timeline-date">10th September 2025 - Wednesday</h4>

          <div class="timeline-event plenary">
            <span class="time">09:10 - 10:00 BST</span>
            <span class="desc">
              <div class="plenary-speaker">
                Plenary Talk - <a href="https://omula.gitlab.io/" target="_blank">Olga Mula</a>
              </div>
              <div class="plenary-title">
                <span class="title-label">Title:</span> Stable Nonlinear Dynamical Approximation with Dynamical Sampling
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  In this talk, I present a strategy to solve time-dependent
                  PDEs with nonlinear dynamical approximation schemes which are provably
                  stable. The stability, and numerical complexity of the approach rely on
                  the ability to evaluate the numerical solution (or some moments of it)
                  at specific locations that evolve in time, and which are chosen in such
                  a way to maximize the stability of the method. I will explain the
                  concept of stability that is involved as well as a possible approach for
                  the dynamical sampling. I will illustrate the behavior of the method in
                  several numerical examples.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">10:00 - 10:30 BST</span>
            <span class="desc">Coffee Break</span>
          </div>

          <div class="timeline-event talk">
            <span class="time">10:30 - 11:00 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://sites.google.com/view/anshimasingh" target="_blank">Anshima Singh</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Wavelet-based Physics-Informed Neural Networks
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Physics-informed neural networks (PINNs) face significant challenges when solving differential equations with rapid oscillations, steep gradients, or singular behavior. 
                  Considering these challenges, we designed an efficient wavelet-based PINNs (W-PINNs) model to address this class of differential equations. We represent the solution in wavelet space using a family of smooth-compactly supported wavelets. 
                  This approach captures the dynamics of complex physical phenomena with significantly fewer degrees of freedom, enabling faster and more accurate training by searching for solutions within the wavelet space. 
                  Developed model eliminates the need for automatic differentiation of derivatives in differential equations and requires no prior knowledge about solution behavior, such as the location of abrupt features. 
                  In this talk, I will demonstrate how W-PINNs excel at capturing localized nonlinear information through a strategic fusion of wavelets with PINNs, making them particularly effective for problems exhibiting abrupt behavior in specific regions. 
                  Our experimental findings show that W-PINNs significantly outperform traditional PINNs, PINNs with wavelets as an activation function, and other state-of-the-art methods, offering a promising approach for tackling challenging differential equations in scientific and engineering applications.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event talk">
            <span class="time">11:00 - 11:30 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="javascript:void(0)">Benno Huber</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Low-rank surrogates for parametric PDEs
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Mathematically, operator surrogates provide approximations of mappings between infinite dimensional spaces. We analyze an encoder-decoder framework, where the input and output function spaces are parametrized using coefficient sequences of admissable representation systems.
                  The goal then becomes to approximate a coefficient-to-coefficient map, which can be realized using various approximation tools.
                  In the present work we focus on low-rank tensor representation using the tensor-train (TT) format. This format allows for efficient storage, evaluation and basic linear algebra as long as the tensor ranks remain low.
                  We show approximation rates with regards to the storage complexity of such TT surrogates for certain holomorphic maps by providing rank bounds. Furthermore, we draw comparisons to other approximation tools such as neural networks and sparse polynomial interpolation.
                </p>
              </div>
            </span>
          </div>


          <div class="timeline-event talk">
            <span class="time">11:30 - 12:00 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://scholar.google.com/citations?user=FZEeaOIAAAAJ&hl=it" target="_blank">Nicola Rares Franco</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> The mathematics behind deep autoencoders: what we know and what is next
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Deep autoencoders have become a fundamental tool in various machine learning applications, ranging from dimensionality reduction and reduced order modeling of partial differential equations to anomaly detection and neural machine translation. 
                  However, despite their empirical success, the theoretical understanding of these architectures remains relatively underdeveloped, particularly in contrast to classical linear approaches such as Principal Component Analysis. 
                  In this talk, I will overview some of the theory concerning deep autoencoders, addressing different aspects through different mathematical tools. More precisely, I will discuss: (i) the relationship between the latent dimension of deep autoencoders and some results in topology and dimension theory, such as the Borsuk-Ulam theorem and the Menger-Nöbeling theorem; (ii) the extension of such results to the context of stochastic systems, where Gaussian processes enter into play; (iii) some results about the expressivity of deep autoencoders, specifically focusing on convolutional architectures and, finally, (iv) some recent advancements concerning the design and optimization of symmetric architectures inspired by the Eckart-Young theorem. 
                  Theoretical results will be accompanied by suitable numerical experiments, illustrating the interplay between theory and practice.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">12:00 - 13:30 BST</span>
            <span class="desc">Lunch</span>
          </div>

          <div class="timeline-event plenary">
            <span class="time">13:30 - 14:20 BST</span>
            <span class="desc">
              <div class="plenary-speaker">
                Plenary Talk - <a href="https://www.kth.se/profile/rvinuesa/?l=en" target="_blank">Ricardo Vinuesa (online)</a>
              </div>
              <div class="plenary-title">
                <span class="title-label">Title:</span> Improving turbulence control through explainable deep learning
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  In this work we first use explainable deep learning based on Shapley explanations to identify the most important regions for predicting the future states of a turbulent channel flow. 
                  The explainability framework (based on gradient SHAP) is applied to each grid point in the domain, and through percolation analysis we identify coherent flow regions of high importance. 
                  These regions have around 70% overlap with the intense Reynolds-stress (Q) events in two-dimensional vertical planes. Interestingly, these importance-based structures have high overlap with classical turbulence structures (Q events, streaks and vortex clusters) in different wall-normal locations, suggesting that this new framework provides a more comprehensive way to study turbulence. 
                  We also discuss the application of deep reinforcement learning (DRL) to discover active-flow-control strategies for turbulent flows, including turbulent channels, three-dimensional cylinders and turbulent separation bubbles. 
                  In all the cases, the discovered DRL-based strategies significantly outperform classical flow-control approaches. We conclude that DRL has tremendous potential for drag reduction in a wide range of complex turbulent-flow configurations.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event break">
            <span class="time">14:20 - 14:50 BST</span>
            <span class="desc">Coffee Break</span>
          </div>

          <div class="timeline-event talk">
            <span class="time">14:50 - 15:20 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://scholar.google.com/citations?user=ezCkT1QAAAAJ&hl=en" target="_blank">Sivalingam S M (online)</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Introduction to Fractional Dynamics and Learning Approaches
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Many natural and engineered systems remember their past, for example, materials that slowly deform, fluids with long-lasting vortices, or biological populations affected by historical conditions. 
                  Traditional models often struggle to capture this memory effect, but fractional differential equations provide a powerful mathematical framework. In this talk, I will introduce fractional dynamics using simple examples to show how past states influence present behaviour. 
                  Using visualizations and simulations, I will demonstrate how learning-based methods, such as neural networks, can efficiently approximate these systems. While the focus is on conceptual understanding, I will also briefly highlight how these methods connect to advanced research areas, such as operator learning, neural PDE solvers, and physics-informed learning. 
                  The goal is to provide a clear understanding of memory-dependent systems while presenting a path to more advanced modelling techniques.
                </p>
              </div>
            </span>
          </div>

          <div class="timeline-event talk">
            <span class="time">15:20 - 15:50 BST</span>
            <span class="desc">
              <div class="talk-speaker">
                Talk - <a href="https://eviatarbach.com/" target="_blank">Eviatar Bach (online)</a>
              </div>
              <div class="talk-title">
                <span class="title-label">Title:</span> Learning probabilistic filters for data assimilation
              </div>
              <button class="toggle-abstract">Show Abstract</button>
              <div class="abstract">
                <p>
                  Filtering - the task of estimating the conditional distribution for states of a dynamical system given partial and noisy observations - is important in many areas of science and engineering, including weather and climate prediction. However, the filtering distribution is generally intractable to obtain for high-dimensional, nonlinear systems. 
                  Filters used in practice, such as the ensemble Kalman filter (EnKF), provide biased probabilistic estimates for nonlinear systems and have numerous tuning parameters. I will present a framework for learning a parameterised analysis map - the transformation that takes samples from a forecast distribution, and combines with an observation, to update the approximate filtering distribution - using variational inference. 
                  In principle this can lead to a better approximation of the filtering distribution, and hence smaller bias. We show that this methodology can be used to learn the gain matrix, in an affine analysis map, for filtering linear and nonlinear dynamical systems; we also study the learning of inflation and localisation parameters for an EnKF. 
                  The framework developed here can also be used to learn new filtering algorithms with more general forms for the analysis map. I will also present some recent work on learning corrections to the EnKF using permutation-invariant neural architectures, leading to superior performance compared to leading methods in filtering chaotic systems.
                </p>
              </div>
            </span>
          </div>
        </div>

      </div>
    </section>




    <!-- ======= Plenary Speakers Section ======= -->
    <section id="speakers" class="portfolio bg-speakers">
      <div class="container">
        <div class="section-title">
          <h2>Plenary Speakers</h2>
        </div>

        <div class="row justify-content-center">
          
          <!-- Speaker 1 -->
          <div class="col-md-3 text-center">
            <!-- <img src="assets/img/OlgaFink.png" alt="Olda Fink" class="m-2 crop-img"> -->
            <div>
              <a href="https://people.epfl.ch/olga.fink?lang=en" target="_blank">Olga Fink</a>
            </div>
            <div>
              EPFL, Switzerland
            </div>
          </div>

          <!-- Speaker 2 -->
          <div class="col-md-3 text-center">
            <!-- <img src="assets/img/OlgaMula.png" alt="Olga Mula" class="m-2 crop-img"> -->
            <div>
              <a href="https://omula.gitlab.io/" target="_blank">Olga Mula</a>
            </div>
            <div>
              University of Vienna, Austria
            </div>
          </div>

          <!-- Speaker 3 -->
          <div class="col-md-3 text-center">
            <!-- <img src="assets/img/George.png" alt="George Stepaniants" class="m-2 crop-img"> -->
            <div>
              <a href="https://georgestepaniants.com/" target="_blank">George Stepaniants</a>
            </div>
            <div>
              California Institute of Technology, US
            </div>
          </div>

          <!-- Speaker 4 -->
          <div class="col-md-3 text-center">
            <!-- <img src="assets/img/Ricardo.png" alt="Ricardo Vinuesa" class="m-2 crop-img"> -->
            <div>
              <a href="https://www.kth.se/profile/rvinuesa/?l=en" target="_blank">Ricardo Vinuesa (online)</a>
            </div>
            <div>
              University of Michigan, US
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- Plenary Speakers Section -->



    <!-- ======= Talks Section ======= -->
    <section id="talks" class="portfolio bg-talks">
      <div class="container">
        <div class="section-title">
          <h2>Talks</h2>
        </div>

        <div class="row">
          <div class="col-md-6">
            <ul>
              <li>
                <a href="https://eviatarbach.com/" target="_blank">Eviatar Bach (online)</a><br>
                University of Reading, UK
              </li>
              <li>
                <a href="https://scholar.google.com/citations?user=FZEeaOIAAAAJ&hl=it" target="_blank">Nicola Rares Franco</a><br>
                MOX, Politecnico di Milano, Italy
              </li>
              <li>
                <a href="https://scholar.google.com/citations?user=jcabNR0AAAAJ&hl=en" target="_blank">Fernando Henríquez</a><br>
                TU Wien, Austria
              </li>
              <li>
                <a href="javascript:void(0)">Benno Huber</a><br>
                Heidelberg University, Germany
              </li>
              <li>
                <a href="https://dibyakanti.github.io/" target="_blank">Dibyakanti Kumar</a><br>
                The University of Manchester, UK
              </li>
              <li>
                <a href="https://scholar.google.com/citations?user=ezCkT1QAAAAJ&hl=en" target="_blank">Sivalingam S M (online)</a><br>
                NIT Puducherry, India
              </li>
            </ul>
          </div>
          <div class="col-md-6">
            <ul>
              <li>
                <a href="https://romit-maulik.github.io/index.html" target="_blank">Romit Maulik (online)</a><br>
                Pennsylvania State University, US &<br> Argonne National Laboratory, US
              </li>
              <li>
                <a href="https://scholar.google.com/citations?user=DN9CCpkAAAAJ&hl=sr" target="_blank">Bogdan Raonić (online)</a><br>
                ETH Zürich, Switzerland
              </li>
              <li>
                <a href="javascript:void(0)">Niklas Reinhardt</a><br>
                Heidelberg University, Germany
              </li>
              <li>
                <a href="https://tomoleary.github.io" target="_blank">Thomas O'Leary-Roseberry</a><br>
                UT Austin, US
              </li>
              <li>
                <a href="https://sites.google.com/view/anshimasingh" target="_blank">Anshima Singh</a><br>
                The University of Manchester, UK
              </li>
              <!-- <li class="text-muted fst-italic mt-2">
                More speakers will be announced soon!
              </li> -->
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Talks Section -->



    <!-- ======= Our Portfolio Section ======= -->
    <section id="team" class="about">
      <div class="container">
        <div class="section-title">
          <h2>Organizers</h2>
        </div>
        <div class="row justify-content-center">
          <!-- Organizer 1 -->
          <div class="col-md-4 text-center">
            <img src="assets/img/picAnirbit.png" alt="Anirbit Mukherjee" class="m-2 crop-img">
            <div>
              <a href="https://anirbit-ai.github.io/" target="_blank">Anirbit Mukherjee</a>
            </div>
            <div>
              University of Manchester, United Kingdom
            </div>
          </div>

          <!-- Organizer 2 -->
          <div class="col-md-4 text-center">
            <img src="assets/img/jakob.png" alt="Jakob Zech" class="m-2 crop-img">
            <div>
              <a href="https://jakobzech.com/" target="_blank">Jakob Zech</a>
            </div>
            <div>
              Heidelberg University, Germany
            </div>
          </div>
        </div>

        <br><br>

        <div class="section-title">
          <h2>Co-organizers</h2>
        </div>
        <div class="row justify-content-center">
          <!-- Co-organizer 1 -->
          <div class="col-md-4 text-center">
            <!-- <div><strong>Co-organizer</strong></div> -->
            <img src="assets/img/Mauricio.png" alt="Mauricio A Álvarez" class="m-2 crop-img">
            <div>
              <a href="https://maalvarezl.github.io/" target="_blank">Mauricio A Álvarez</a>
            </div>
            <div>
              University of Manchester, United Kingdom
            </div>
          </div>

          <!-- Co-organizer 2 -->
          <div class="col-md-4 text-center">
            <!-- <div><strong>Co-organizer</strong></div> -->
            <img src="assets/img/Tobias.png" alt="Tobias Buck" class="m-2 crop-img">
            <div>
              <a href="https://tobias-buck.de/" target="_blank">Tobias Buck</a>
            </div>
            <div>
              Heidelberg University, Germany
            </div>
          </div>

          <!-- Panel Discussion Organizer -->
          <!-- <div class="col-md-3 text-center">
            <div><strong>Panel Discussion Organizer</strong></div>
            <img src="assets/img/Eviatar.png" alt="Eviatar Bach" class="m-2 crop-img">
            <div>
              <a href="https://eviatarbach.com/" target="_blank">Eviatar Bach</a>
            </div>
            <div>
              University of Reading, United Kingdom
            </div>
          </div> -->
        </div>

        <br><br>

        <div class="section-title">
          <h2>Student Volunteer</h2>
        </div>
        <div class="row justify-content-center">
          <div class="col-md-6 text-center">
            <img src="assets/img/Dibyakanti.png" alt="Dibyakanti Kumar" class="m-2 crop-img">
            <div>
              <a href="https://dibyakanti.github.io/" target="_blank">Dibyakanti Kumar</a>
            </div>
            <div>
              University of Manchester, United Kingdom
            </div>
          </div>
        </div>
      </div>
    </section>






    <!-- ======= Frequently Asked Questions Section ======= -->

    <!-- <section id="Registration" class="portfolio section-bg">
          <div class="container">
            <div class="section-title">
              <h2>Registration Link</h2>
            </div>
            <div class="row">
              <div class="col-lg-12 text-center">
                <p>Kindly register for the event through the following <a
                    href="https://www.ticketsource.co.uk/nlp4socialgood">link</a>.
                  Registration is open until 24th April, 2024.</p>

              </div>
            </div>
          </div>
        </section> -->
    <!-- End Our Portfolio Section -->



    <!-- ======= Our Portfolio Section ======= -->
    <section id="location" class="portfolio section-bg ">
      <div class="container">
        <div class="section-title">
          <h2>Location</h2>
        </div>
        <div class="row">
          <div class="col-lg-12 d-flex justify-content-center align-items-center" style="min-height: 80px;">
            Room 2B.026, Engineering Building B, University of Manchester, United Kingdom
            <br>
            This will be a hybrid event. Zoom links will be available on the page for joining during the event.
          </div>
        </div>
      </div>
    </section>
    <!-- End Our Portfolio Section -->

    <!-- ======= Previous Events Section ======= -->
    <!-- <section id="previous-events" class="previous-events section-bg">
          <div class="container">
            <div class="section-title">
              <h2>Previous Events: NSG-2023</h2>
            </div>
            <div class="row">
              <div class="col-lg-12 text-center">
                <a href="https://www.youtube.com/@NLPforSocialGood" class="btn-watch-videos" target="_blank">Watch
                  Videos</a>
              </div>
            </div>
          </div>
        </section> -->
    <!-- End Previous Events Section -->

    <!-- ======= Our Team Section ======= -->
    <section id="contact" class="team section-bg ">
          <div class="container">
            <div class="section-title">
              <h2>Contact Us</h2>
              <p>Please reach out to the organizers for any questions via this email: <i>anirbit.mukherjee@manchester.ac.uk</i>
              </p>
            </div>
          </div>
        </section>
    </div>
    </section>
  </main>
  <!-- End #main -->

  <!-- ======= Footer ======= -->
  
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        Supported by the <a href="https://www.ai-fun.manchester.ac.uk/" target="_blank">Centre for AI Fundamentals</a>
      </div>
    </div>
  </footer>
      <!-- <div class="credits"> -->
  <!-- All the links in the footer should remain intact. -->
  <!-- You can delete the links only if you purchased the pro version. -->
  <!-- Licensing information: https://bootstrapmade.com/license/ -->
  <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/free-one-page-bootstrap-template-amoeba/ -->
  <!-- Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div> -->
  <!-- End #footer -->

  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

  <script src="assets/js/abstract.js"></script>
</body>

</html>